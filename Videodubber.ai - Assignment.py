{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n!pip install firecrawl-py\n!pip install mailjet_rest\n!pip install llama_index\n!pip install llama-index llama-index-llms-groq groq\nimport requests\nimport time\nimport re\nfrom firecrawl import FirecrawlApp\nfrom requests.exceptions import HTTPError\nfrom llama_index.llms.groq import Groq\nimport textwrap\nfrom mailjet_rest import Client\nimport os\n\n# Initialize the Groq LLM\nllm_groq = Groq(model=\"llama3-70b-8192\", api_key=\"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\")  # Replace with your actual API key\n\n# Function to fetch search results\ndef fetch_search_results(query):\n    api_key = '4923928f27936aada0d11cde8420f22cef39a25bfa859e189de2e1e97d837ba4'  # Replace with your actual API key\n    url = f\"https://serpapi.com/search?api_key={api_key}&q={query}\"\n    \n    response = requests.get(url)\n    \n    if response.status_code == 200:\n        results = response.json()\n        return results.get('organic_results', [])[:10]  # Get top 10 results\n    else:\n        print(\"Error fetching results:\", response.status_code)\n        return []\n\n# Function to clean Website content\ndef clean_wikipedia_content(content):\n    if not isinstance(content, str):\n        content = str(content)\n        \n    cleaned_content = re.sub(r'\\[.*?\\]', '', content)  # Remove [text]\n    cleaned_content = re.sub(r'https?://\\S+', '', cleaned_content)  # Remove URLs\n    cleaned_content = re.sub(r'!\\(data:image/\\S+;base64,[^\\)]+\\)', '', cleaned_content)  # Remove image data URLs\n    cleaned_content = re.sub(r'::+', ' ', cleaned_content)  # Replace multiple colons with space\n    cleaned_content = re.sub(r'\\s+', ' ', cleaned_content)  # Replace multiple whitespace with single space\n    \n    return cleaned_content.strip()\n\n# Function to split content into chunks\ndef split_content_into_chunks(content, chunk_size=15000):\n    return textwrap.wrap(content, chunk_size)\n\n# Function to summarize each chunk with Groq\ndef summarize_with_groq(text):\n    prompt = f\"\"\"\n    Generate a summary of the text which explains everything in 200 words at maximum. Do not exceed this limit.\n    \n    Text: {text}\n    \"\"\"\n    \n    response = llm_groq.complete(prompt)\n    return response.text  # Adjust according to the response format of Groq\n\n# Function to scrape content from URLs with rate limit handling\ndef scrape_urls(urls, firecrawl):\n    all_summaries = []\n    \n    for url in urls:\n        while True:\n            try:\n                print(f\"Scraping {url}...\")\n                \n                # Scrape the URL and get the content\n                page_content = firecrawl.scrape_url(url=url, params={\"pageOptions\": {\"onlyMainContent\": True}})\n                \n                # Ensure the content is a string\n                if not isinstance(page_content, str):\n                    page_content = str(page_content)\n                \n                # Clean the content\n                cleaned_content = clean_wikipedia_content(page_content)\n                \n                # Split the cleaned content into chunks\n                chunks = split_content_into_chunks(cleaned_content)\n                \n                # Summarize each chunk using Groq\n                chunk_summaries = [summarize_with_groq(chunk) for chunk in chunks]\n                \n                # Combine chunk summaries\n                combined_summary = \" \".join(chunk_summaries)\n                \n                # Add this URL's combined summary to the list of all summaries\n                all_summaries.append(combined_summary)\n                \n                # Print the URL's combined summary\n                print(f\"Summary from {url}:\")\n                print(combined_summary)\n                print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n                break  # Exit the loop if scraping is successful\n            \n            except HTTPError as e:\n                if e.response.status_code == 429:\n                    # Handle rate limit exceeded error\n                    reset_time = e.response.headers.get('Retry-After', 45)  # Default to 45 seconds if not provided\n                    print(f\"Rate limit exceeded. Pausing for {reset_time} seconds...\")\n                    time.sleep(int(reset_time))  # Pause the script\n                else:\n                    print(f\"An error occurred: {e}\")\n                    break  # Exit the loop for other errors\n\n    # Combine all URL summaries\n    large_text = \" \".join(all_summaries)\n    \n    # Create the prompt for the final summarization\n    final_prompt = f\"\"\"\n    Based on the following text, generate:\n    1. A title\n    2. An introduction\n    3. Five top subtopic titles\n    4. A short summary\n    5. A conclusion\n\n    Text: {large_text}\n    \"\"\"\n    \n    # Generate the final summary with Groq\n    final_summary = llm_groq.complete(final_prompt)\n    \n    # Print the final summary\n    print(\"Final Summary of All URLs:\")\n    print(final_summary)\n\n    return final_summary.text  # Return final summary for email\n\n# Function to send an email using Mailjet\ndef send_email_mailjet(subject, body, recipient, rec_name):\n    api_key = 'fd5ec7040540bf0b2ffe1f6d5bcc49aa'\n    api_secret = \"347f162716754df8ba0d497abb8f5c4b\"\n    \n    mailjet = Client(auth=(api_key, api_secret), version='v3.1')\n    \n    data = {\n        'Messages': [\n            {\n                'From': {\n                    'Email': 'nrj.eea@gmail.com',\n                    'Name': 'Neeraj'\n                },\n                'To': [\n                    {\n                        'Email': recipient,\n                        'Name': rec_name\n                    }\n                ],\n                'Subject': subject,\n                'TextPart': body,\n                'HTMLPart': f\"<p>{body}</p>\"  # HTML format for email body\n            }\n        ]\n    }\n    \n    result = mailjet.send.create(data=data)\n    print(result.status_code)\n    print(result.json())\n\n# Initialize the Firecrawl application with your API key\napi_key = \"fc-0d7421cf1f3c467aa17269df26a307e8\"  # Replace with your actual API key\nfirecrawl = FirecrawlApp(api_key=api_key)\n\n# Step 1: Fetch search results\nquery = input(\"Enter your search query: \")\nresults = fetch_search_results(query)\n\n# Step 2: Extract URLs from the search results\nurls = [result.get('link') for result in results if result.get('link')]\n\n# Step 3: Scrape content from the URLs\nfinal_summary = scrape_urls(urls, firecrawl)\n\n# Step 4: Send the final summary via email\nrecipient_email = input(\"Enter recipient email address: \")\nrec_name = input(\"Enter the name: \")\nsend_email_mailjet(\"Final Summary of Search Results\", final_summary, recipient_email, rec_name)\n","metadata":{"_uuid":"c178dbd9-4832-4661-8f32-a3e543f71a60","_cell_guid":"ab0113de-e32e-47dc-9615-31486042d8d7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}